# @package algorithm

_target_: src.algorithms.vicreg.VICReg


masking: null


feature_blur:
  _target_: src.models.augmentation.FastFeatureBlur
  prob: 0.9004016520359364
  magnitude: 0.92685112547458
  strength: 0.749655944679925


object_mask:
  _target_: src.models.augmentation.FastObjectMask
  prob: 0.5709886168264394


lorentz_rotation:
  _target_: src.models.augmentation.FastLorentzRotation
  prob: 0.5
  norm_scale: 0.1
  norm_bias: 0.1
  phi_indices: null


model:
  _target_: src.models.mlp.MLP

  nodes: [57, 29, 10]

  batchnorm: false

  init_weight:
    _target_: src.utils.hydra.get_method
    path: torch.nn.init.xavier_uniform_

  init_bias:
    _target_: src.utils.hydra.get_method
    path: torch.nn.init.zeros_

projector:
  _target_: src.models.mlp.MLP
  nodes: [10, 40] # * 4 projection
  batchnorm: true # Added batchnorm


loss:
  _target_: src.losses.vicreg.L1VICRegLoss
  inv_coef: 50
  var_coef: 50
  cov_coef: 1


optimizer:
  _partial_: true
  _target_: torch.optim.Adam
  lr: 0.0001033146150433934
  weight_decay: 0.0


scheduler:
  # here other attributes...

  scheduler:
    _partial_: true
    _target_: src.optimizers.cdw.CosineWithWarmup
    warmup_epochs: 10
    decay_epochs: '${substract: ${trainer.max_epochs}, ${algorithm.scheduler.scheduler.warmup_epochs}}'
    max_lr: ${algorithm.optimizer.lr}
