# Quantised VAE trained on top of vicreg output.
defaults:
  - _self_

_target_: src.algorithms.vae.VAE
features:
  _target_: src.algorithms.components.features.FeaturesFromCkpt
  ckpt_path: ???
  attr: model
  litmodule_cls:
    _target_: hydra.utils.get_class
    path: src.algorithms.vicreg.VICReg

mask: false

encoder:
  _target_: src.algorithms.components.encoder.hgq_variational_encoder
  in_dim: 10
  nodes: [9, 6]
  out_dim: 4
  output_layer_config:
    place: 'all'
    q_type: "kif"
    i0: 5
    f0: 3
    trainable: false
    ic:
      _target_: hgq.constraints.Constant
      value: 5
    fc:
      _target_: hgq.constraints.Constant
      value: 3
  ebops: true

decoder:
  _target_: src.algorithms.components.decoder.hgq_decoder
  in_dim: ${algorithm.encoder.out_dim}
  nodes: [6, 9]
  out_dim: ${algorithm.encoder.in_dim}

  output_layer_config:
    place: 'all'
    q_type: "kif"
    i0: 5
    f0: 3
    trainable: false
    ic:
      _target_: hgq.constraints.Constant
      value: 5
    fc:
      _target_: hgq.constraints.Constant
      value: 3
  ebops: true

loss:
  _target_: src.losses.vae.ClassicVAELoss
  scale: 1
  kl_scale: 0.001

# Annealing of the KL scale scalar.
kl_warmup_frac: 0.2

optimizer:
  _partial_: true
  _target_: torch.optim.AdamW
  lr: 0.001
  weight_decay: 0.0
  betas: [0.9, 0.999]
  eps: 1e-8
