defaults:
  - partitions: axov4

_target_: src.data.L1SL_datamodule.L1SLDatamodule

# Fill in below with paths to folders that contain the raw data in h5 format.
zerobias: ${paths.zerobias}
signal: ${paths.signal}
background: ${paths.background}

# Extract only the relevant data to use in the processing from the h5 files.
data_extractor:
  _target_: src.data.components.extraction.L1DataExtractor
  # Choose which objects to use and their corresponding features.
  objects_features:
    muons: ['muonIEt', 'muonIEta', 'muonIPhi']
    egammas: ['egIEt', 'egIEta', 'egIPhi']
    jets: ['jetIEt', 'jetIEta', 'jetIPhi']
    ET: ['Et']
    MET: ['Et', 'phi']
  # For the particle objects, choose which constituents to use.
  constituents:
    muons: [True, True, True, True, False, False, False, False]
    egammas: [True, True, True, True, False, False, False, False, False, False, False, False]
    jets: [True, True, True, True, True, True, True, True, True, True, False, False]
  cache: ${paths.data_dir}/extracted
  name: axol1tl

# Processing of the data.
# Events saturated in the pT/transverse energy are masked/removed.
# Specify which objects/features to remove from the data.
data_processor:
  _target_: src.data.components.processing.L1DataProcessor
  extracted_path: ${data.data_extractor.cache}/${data.data_extractor.name}
  remove_objects_features:
    ET: ['Et']
  cache: ${paths.data_dir}/processed
  name: axol1tl


classifier_signals:
  - GluGluHToBB_M-125
  - GluGluHToGG_M-125
  - GluGluHToGG_M-90
  - GluGluHToTauTau_M-125
  - GluGlutoHHto2B2WtoLNu2Q_kl-1p00_kt-1p00_c2-0p00
  - HHHTo6B_c3_0_d4_0
  - HHHto4B2Tau_c3-0_d4-0
  - HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-100000mm
  - HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-10000mm
  - HTo2LongLivedTo4b_MH-125_MFF-12_CTau-900mm
  - HTo2LongLivedTo4b_MH-125_MFF-25_CTau-1500mm
  - HTo2LongLivedTo4b_MH-125_MFF-50_CTau-3000mm
  - SMS-Higgsino_mN2-170_mC1-160_mN1-150_HT-60
  - SMS-Higgsino_mN2-170_mC1-160_mN1-150_HT60
  - SUSYGluGluToBBHToBB_NarrowWidth_M-1200_1
  - SUSYGluGluToBBHToBB_NarrowWidth_M-1200_2
  - SUSYGluGluToBBHToBB_NarrowWidth_M-120_1
  - SUSYGluGluToBBHToBB_NarrowWidth_M-120_2
  - SUSYGluGluToBBHToBB_NarrowWidth_M-350_1
  - SUSYGluGluToBBHToBB_NarrowWidth_M-350_2
  - SUSYGluGluToBBHToBB_NarrowWidth_M-600_1
  - SUSYGluGluToBBHToBB_NarrowWidth_M-600_2
  - VBFHToCC_M-125
  - VBFHToInvisible_M-125
  - VBFHToTauTau_M125
  - VBFHto2B_M-125
  - WToTauTo3Mu
  - WToTauToMuMuMu
  - Wlnu
  - ggXToJpsiJpsiTo2Mu2E_m7_pseudoscalar
  - ggXToYYTo2Mu2E_m14_pseudoscalar
  - ggXToYYTo2Mu2E_m18_pseudoscalar
  - ggXToYYTo2Mu2E_m26_pseudoscalar
  - haa-4b-ma15
  - ttHto2B_M-125
  - ttHto2C_M-125

# Normalize the data.
# Choose which type of normalization to use and give hyperparameters of that
# normalization scheme, if any.
data_normalizer:
  _target_: src.data.components.normalization.L1DataNormalizer
  norm_scheme: changrobust
  norm_hyperparams:
    percentiles: [95, 5]
    scale: [2, -2]
  ignore_zeros: True
  output_dtype: float32
  cache: ${paths.data_dir}/normed/allsig_classification
  processed_data_folder: ${data.data_processor.cache}/${data.data_processor.name}

# Split the main data set into 80% training, 10% validation, 10% test.
split: [0.8, 0.1, 0.1]

# Parameters specifying how and where the data is loaded.
batch_size: 16384
val_batches: 10
