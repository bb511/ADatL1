_target_: src.models.vicreg.VICReg

feature_blur:
  _target_: src.models.components.vicreg.FastFeatureBlur
  prob: 0.9004016520359364
  magnitude: 0.92685112547458
  strength: 0.749655944679925

object_mask:
  _target_: src.models.components.vicreg.FastObjectMask
  prob: 0.5709886168264394

lorentz_rotation:
  _target_: src.models.components.vicreg.FastLorentzRotation
  prob: 0.5
  norm_scale: ???
  norm_bias: ???
  phi_indices: null

qdata: null
  # _target_: src.models.quantization.new_quantizer
  # method: fixed-point
  # bits: 8
  # integer: 5

model:
  _target_: src.models.components.qmlp.QMLP
  nodes: [57, 29, 10] # the first one is an input
  batchnorm: false

  qweight:
    _target_: src.models.quantization.new_quantizer
    method: fixed-point
    bits: 6
    integer: 2

  qbias:
    _target_: src.models.quantization.new_quantizer
    method: fixed-point
    bits: 10
    integer: 6

  qactivation:
    _target_: src.models.quantization.new_quantizer
    method: fixed-point
    bits: 10
    integer: 6

  init_weight:
    _target_: src.utils.hydra.get_method
    path: torch.nn.init.xavier_uniform_

  init_bias:
    _target_: src.utils.hydra.get_method
    path: torch.nn.init.zeros_

projector:
  _target_: src.models.components.qmlp.QMLP
  nodes: [10, 40] # * 4 projection
  batchnorm: true # Added batchnorm

  # No quantization or initialization:
  qweight: null
  qbias: null
  qactivation: null

  # No initialization:
  init_weight: null
  init_bias: null

  
loss:
  _target_: src.models.losses.vicreg.L1VICRegLoss
  inv_coef: 50
  var_coef: 50
  cov_coef: 1


  # self, norm_scales: np.ndarray, norm_biases: np.ndarray, 
  #                mask: Dict[str, np.ndarray], unscale_energy: bool = False):

optimizer:
  _partial_: true
  _target_: torch.optim.Adam
  lr: 0.0001033146150433934
  weight_decay: 0.0

scheduler:
  # here other attributes...

  scheduler:
    _partial_: true
    _target_: src.models.optimizers.cdw.CosineWithWarmup
    warmup_epochs: 10
    decay_epochs: '${substract: ${trainer.max_epochs}, ${model.scheduler.scheduler.warmup_epochs}}'
    max_lr: ${model.optimizer.lr}
